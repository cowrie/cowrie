{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "408c1a12-df24-4703-9491-3a4bb3face4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install accelerate\n",
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7a3959d-5e81-4a98-a0b2-dcb0c69563da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f783ca65-f78e-43f3-8629-462ee07100c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available. PyTorch can access the GPU.\n"
     ]
    }
   ],
   "source": [
    "#import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available. PyTorch can access the GPU.\")\n",
    "else:\n",
    "    print(\"GPU is not available. PyTorch cannot access the GPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b179ccf-30ba-4384-b971-3254695bdd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.1\n",
      "CUDA version: 12.1\n",
      "cuDNN version: 8907\n",
      "GPU is available. PyTorch can access the GPU.\n",
      "Number of GPUs available: 1\n",
      "GPU 0: NVIDIA GeForce RTX 4080 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available. PyTorch can access the GPU.\")\n",
    "    print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"GPU is not available. PyTorch cannot access the GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a768637-e521-4e40-96cb-1409645be8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567159a06fd44bdfb8132721c0f547cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "token = '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'\n",
    "\n",
    "model_name = \"google/codegemma-7b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=token, device_map=\"auto\", load_in_8bit=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8c36d97-b63c-4744-9c27-350905a4d27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on GPU: True\n",
      "Tokenizer is on GPU: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if model and tokenizer are on GPU\n",
    "print(\"Model is on GPU:\", next(model.parameters()).is_cuda)\n",
    "print(\"Tokenizer is on GPU:\", next(model.parameters()).is_cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "366a79a3-d4bb-4473-8bb6-725400772f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base prompt with instructions\n",
    "prompt = \"\"\"You are a Linux terminal simulator. Your task is to generate simulated outputs for various Linux commands. \n",
    "You may choose your own linux commands as input. \n",
    "However, make sure that the outputs generated for these commands are as realistic as a linux terminal would generate. Generate outputs and not the explanation of the command\n",
    "Below are examples of different Linux commands and their typical outputs:\n",
    "\n",
    "Example 1:\n",
    "Command: ls\n",
    "Output:\n",
    "file1.txt\n",
    "patient.txt\n",
    "department.txt\n",
    "report.pdf\n",
    "data.csv\n",
    "\n",
    "Example 2:\n",
    "Command: cat patient.txt\n",
    "Output:\n",
    "Patient ID: 12345\n",
    "Name: John Doe\n",
    "Age: 45\n",
    "Condition: Stable\n",
    "\n",
    "Example 3:\n",
    "Command: echo \"Hello, World!\"\n",
    "Output:\n",
    "Hello, World!\n",
    "\n",
    "Example 4:\n",
    "Command: grep \"error\" logfile.txt\n",
    "Output:\n",
    "2023-06-19 14:23:34 ERROR Failed to connect to server\n",
    "2023-06-19 14:24:10 ERROR Timeout occurred\n",
    "\n",
    "Example 5:\n",
    "Command: ps\n",
    "Output:\n",
    "PID TTY          TIME CMD\n",
    " 1234 pts/0    00:00:01 bash\n",
    " 5678 pts/1    00:00:02 python\n",
    " 91011 pts/2    00:00:00 top\n",
    "\n",
    "Now, generate outputs different Linux commands. For each command, provide a realistic output as shown in the examples above.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5112cd09-2e6b-4569-8827-663c556ba5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9362cd8-b334-4006-a22c-97d9423611cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move input_ids to CUDA if available\n",
    "if torch.cuda.is_available():\n",
    "    inputs = {k: v.to('cuda') for k, v in inputs.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7fc82a3-b3cd-4989-aed8-a3e1ccc9f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate outputs from the model with configured parameters\n",
    "outputs = model.generate(inputs['input_ids'], max_length= 3000, num_return_sequences= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40c8f3cd-7a7b-4907-ba91-1b809de721c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the generated outputs\n",
    "generated_sequence = tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2e86cc5d-7231-438a-814a-804991922855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Linux Command Sequence:\n",
      "You are a Linux terminal simulator. Your task is to generate simulated outputs for various Linux commands. \n",
      "You may choose your own linux commands as input. \n",
      "However, make sure that the outputs generated for these commands are as realistic as a linux terminal would generate. Generate outputs and not the explanation of the command\n",
      "Below are examples of different Linux commands and their typical outputs:\n",
      "\n",
      "Example 1:\n",
      "Command: ls\n",
      "Output:\n",
      "file1.txt\n",
      "patient.txt\n",
      "department.txt\n",
      "report.pdf\n",
      "data.csv\n",
      "\n",
      "Example 2:\n",
      "Command: cat patient.txt\n",
      "Output:\n",
      "Patient ID: 12345\n",
      "Name: John Doe\n",
      "Age: 45\n",
      "Condition: Stable\n",
      "\n",
      "Example 3:\n",
      "Command: echo \"Hello, World!\"\n",
      "Output:\n",
      "Hello, World!\n",
      "\n",
      "Example 4:\n",
      "Command: grep \"error\" logfile.txt\n",
      "Output:\n",
      "2023-06-19 14:23:34 ERROR Failed to connect to server\n",
      "2023-06-19 14:24:10 ERROR Timeout occurred\n",
      "\n",
      "Example 5:\n",
      "Command: ps\n",
      "Output:\n",
      "PID TTY          TIME CMD\n",
      " 1234 pts/0    00:00:01 bash\n",
      " 5678 pts/1    00:00:02 python\n",
      " 91011 pts/2    00:00:00 top\n",
      "\n",
      "Now, generate outputs different Linux commands. For each command, provide a realistic output as shown in the examples above.\n",
      "\n",
      "**Command:** pwd\n",
      "**Output:** /home/user/Documents\n",
      "\n",
      "**Command:** mkdir new_folder\n",
      "**Output:**\n",
      "\n",
      "**Command:** cd new_folder\n",
      "**Output:** /home/user/Documents/new_folder\n",
      "\n",
      "**Command:** touch file.txt\n",
      "**Output:**\n",
      "\n",
      "**Command:** rm file.txt\n",
      "**Output:**\n",
      "\n",
      "**Command:** mv file.txt new_file.txt\n",
      "**Output:**\n",
      "\n",
      "**Command:** cp file.txt new_file.txt\n",
      "**Output:**\n",
      "\n",
      "**Command:** ls -l\n",
      "**Output:**\n",
      "\n",
      "**Command:** grep \"hello\" file.txt\n",
      "**Output:**\n",
      "\n",
      "**Command:** head file.txt\n",
      "**Output:**\n",
      "\n",
      "**Command:** tail file.txt\n",
      "**Output:**\n",
      "\n",
      "**Command:** man ls\n",
      "**Output:**\n",
      "\n",
      "**Command:** whoami\n",
      "**Output:** user\n",
      "\n",
      "**Command:** date\n",
      "**Output:** Mon Jun 19 15:00:00 EDT 2023\n",
      "\n",
      "**Command:** uptime\n",
      "**Output:**\n",
      "\n",
      "**Command:** free -m\n",
      "**Output:**\n",
      "\n",
      "**Command:** df -h\n",
      "**Output:**\n",
      "\n",
      "**Command:** top\n",
      "**Output:**\n",
      "\n",
      "**Command:** htop\n",
      "**Output:**\n",
      "\n",
      "**Command:** ps aux\n",
      "**Output:**\n",
      "\n",
      "**Command:** kill 1234\n",
      "**Output:**\n",
      "\n",
      "**Command:** reboot\n",
      "**Output:**\n",
      "\n",
      "**Command:** shutdown -h now\n",
      "**Output:**\n",
      "\n",
      "**Command:** man shutdown\n",
      "**Output:**\n",
      "\n",
      "**Command:** man reboot\n",
      "**Output:**\n",
      "\n",
      "**Command:** man kill\n",
      "**Output:**\n",
      "\n",
      "**Command:** man ps\n",
      "**Output:**\n",
      "\n",
      "**Command:** man top\n",
      "**Output:**\n",
      "\n",
      "**Command:** man htop\n",
      "**Output:**\n",
      "\n",
      "**Command:** man free\n",
      "**Output:**\n",
      "\n",
      "**Command:** man df\n",
      "**Output:**\n",
      "\n",
      "**Command:** man whoami\n",
      "**Output:**\n",
      "\n",
      "**Command:** man date\n",
      "**Output:**\n",
      "\n",
      "**Command:** man uptime\n",
      "**Output:**\n",
      "\n",
      "**Command:** man man\n",
      "**Output:**\n"
     ]
    }
   ],
   "source": [
    "# Print or use the generated sequence\n",
    "print(\"Generated Linux Command Sequence:\")\n",
    "print(generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6a4c06-7bbb-40f9-a465-c42c10bb2050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cuda_env)",
   "language": "python",
   "name": "cuda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
